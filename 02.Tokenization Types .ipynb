{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7abae8-ae97-40c8-9fc5-c2fbdf188902",
   "metadata": {},
   "source": [
    "# Tokenization Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356df54-4470-4e94-8547-4640f7fb6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e496cf8c-0cdc-40b7-aa2f-4dedb72dc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\" Before getting started with the examples, we will set the system up with NLTK and other dependent Python libraries.\n",
    "The pip installer can be used to install NLTK, with an optional installation of numpy, as follows:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08a54e7-2937-442b-bafa-bf5e5f4be293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Before getting started with the examples, we will set the system up with NLTK and other dependent Python libraries.\n",
      "The pip installer can be used to install NLTK, with an optional installation of numpy, as follows:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86baf88-6063-4106-b2cd-81a07d355616",
   "metadata": {},
   "source": [
    "## Keyword Tokerizer\n",
    "- The Natural Language Toolkit (NLTK) provides a convenient way to tokenize text into keywords or tokens.\n",
    "- If you have specific keywords in mind that you want to tokenize around, you might need a custom approach using NLTKâ€™s RegexpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec1a8b8-f565-4d4b-bd56-07c58c8e57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Before', 'getting', 'started', 'with', 'the', 'examples', 'we', 'will', 'set', 'the', 'system', 'up', 'with', 'NLTK', 'and', 'other', 'dependent', 'Python', 'libraries', 'The', 'pip', 'installer', 'can', 'be', 'used', 'to', 'install', 'NLTK', 'with', 'an', 'optional', 'installation', 'of', 'numpy', 'as', 'follows']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Define the keywords as regular expressions\n",
    "keyword_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenize the text\n",
    "keywords = keyword_tokenizer.tokenize(corpus)\n",
    "\n",
    "# Display the keywords\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2be10d-94b9-4d49-9051-bd249c84f2fe",
   "metadata": {},
   "source": [
    "## Character Tokerizer\n",
    "\n",
    "- Character tokenization refers to breaking down a string into individual characters, which can be useful in various natural language processing tasks, especially when working with languages where character-level understanding is important, or for tasks like text generation or analyzing text at a granular level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aca7833-252d-4146-8681-172005836f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'g', 'e', 't', 't', 'i', 'n', 'g', ' ', 's', 't', 'a', 'r', 't', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', 's', ',', ' ', 'w', 'e', ' ', 'w', 'i', 'l', 'l', ' ', 's', 'e', 't', ' ', 't', 'h', 'e', ' ', 's', 'y', 's', 't', 'e', 'm', ' ', 'u', 'p', ' ', 'w', 'i', 't', 'h', ' ', 'N', 'L', 'T', 'K', ' ', 'a', 'n', 'd', ' ', 'o', 't', 'h', 'e', 'r', ' ', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 't', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'l', 'i', 'b', 'r', 'a', 'r', 'i', 'e', 's', '.', '\\n', 'T', 'h', 'e', ' ', 'p', 'i', 'p', ' ', 'i', 'n', 's', 't', 'a', 'l', 'l', 'e', 'r', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 'u', 's', 'e', 'd', ' ', 't', 'o', ' ', 'i', 'n', 's', 't', 'a', 'l', 'l', ' ', 'N', 'L', 'T', 'K', ',', ' ', 'w', 'i', 't', 'h', ' ', 'a', 'n', ' ', 'o', 'p', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'i', 'n', 's', 't', 'a', 'l', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'n', 'u', 'm', 'p', 'y', ',', ' ', 'a', 's', ' ', 'f', 'o', 'l', 'l', 'o', 'w', 's', ':', '\\n', '\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: ' '\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: '\n",
      "'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer\n",
    "\n",
    "# Using the SyllableTokenizer to tokenize into characters\n",
    "char_tokenizer = SyllableTokenizer()\n",
    "\n",
    "# Tokenize text by characters\n",
    "char_tokens = [char for token in char_tokenizer.tokenize(corpus) for char in token]\n",
    "\n",
    "# Display the character tokens\n",
    "print(char_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe10b3-acb1-49cc-a68e-cfe47e24b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9a8f05b-1f39-4e35-b3cf-e5636473d38d",
   "metadata": {},
   "source": [
    "## Classic tokenizer\n",
    "- simple word tokenizer that splits text into tokens (words) based on spaces, punctuation, and other basic delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99317d3-34cd-470a-acc9-d7fd10c74b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Before', 'getting', 'started', 'with', 'the', 'examples', ',', 'we', 'will', 'set', 'the', 'system', 'up', 'with', 'NLTK', 'and', 'other', 'dependent', 'Python', 'libraries.', 'The', 'pip', 'installer', 'can', 'be', 'used', 'to', 'install', 'NLTK', ',', 'with', 'an', 'optional', 'installation', 'of', 'numpy', ',', 'as', 'follows', ':']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Initialize the classic tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(corpus)\n",
    "\n",
    "# Display the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8a4c4-ef03-4338-a9a2-c256acb4b15d",
   "metadata": {},
   "source": [
    "## lowercase tokenizer\n",
    "- A lowercase tokenizer is a variant of a tokenizer that not only splits the text into tokens (words, characters, etc.) but also converts all tokens to lowercase. \n",
    "- This can be useful in natural language processing (NLP) tasks where you want to treat words in a case-insensitive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f4fd4d-a295-4255-b3d6-9e46159760ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['before', 'getting', 'started', 'with', 'the', 'examples', ',', 'we', 'will', 'set', 'the', 'system', 'up', 'with', 'nltk', 'and', 'other', 'dependent', 'python', 'libraries', '.', 'the', 'pip', 'installer', 'can', 'be', 'used', 'to', 'install', 'nltk', ',', 'with', 'an', 'optional', 'installation', 'of', 'numpy', ',', 'as', 'follows', ':']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize and convert to lowercase\n",
    "tokens = [token.lower() for token in word_tokenize(corpus)]\n",
    "\n",
    "# Display the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a7ff6-f61a-4b24-a675-a508c25fe29e",
   "metadata": {},
   "source": [
    "## Whitespace Tokenizer:\n",
    "- Splits text based on whitespace (spaces, tabs, newlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601bbe48-38a7-4d90-831c-9a61b21c9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenizer: ['Hello', 'World,', 'Welcome', 'to', 'new', 'chapter', 'in', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "text = \"    Hello World, Welcome to new    chapter in NLP\"\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
    "print(\"Whitespace Tokenizer:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0807d91-2b6c-4591-a884-d27c0d160fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b695af07-f1b1-4de8-bd72-b65a9a9d8cad",
   "metadata": {},
   "source": [
    "## PunktSentenceTokenizer\n",
    "- A sentence tokenizer that doesn't rely on a pre-defined list of abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcebc315-f0fc-4977-a797-fcc597748ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt Sentence Tokenizer: [' Hello!', 'All, This is Vamsidhar Reddy.', 'I am working as a data scientist.', 'I have a good experience in Data Science']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "text = \" Hello! All, This is Vamsidhar Reddy. I am working as a data scientist. I have a good experience in Data Science\" \n",
    "\n",
    "punkt_tokenizer = PunktSentenceTokenizer()\n",
    "punkt_tokens = punkt_tokenizer.tokenize(text)\n",
    "print(\"Punkt Sentence Tokenizer:\", punkt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf387e9-e668-4a8c-a533-4fa04f043fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
